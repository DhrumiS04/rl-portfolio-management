{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk-Sensitive Portfolio Management with Reinforcement Learning\n",
    "# ====================================================\n",
    "#\n",
    "# This notebook demonstrates:\n",
    "# 1. Policy Gradient (REINFORCE) implementation for portfolio optimization\n",
    "# 2. Actor-Critic (A2C) implementation for improved sample efficiency\n",
    "# 3. Risk-sensitive reward functions incorporating CVaR (Conditional Value-at-Risk)\n",
    "# 4. Performance comparison and stress testing during crisis periods\n",
    "#\n",
    "# Author: Your Name\n",
    "# Date: October 21, 2025\n",
    "\n",
    "# ## 1. Import Libraries and Setup Environment\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Dirichlet\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from tqdm.notebook import tqdm\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "# Create directories for saving models and data\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# ## 2. Data Processing and Financial Metrics\n",
    "\n",
    "# ### 2.1 Data Utilities\n",
    "\n",
    "def download_stock_data(tickers, start_date, end_date, save_path=None):\n",
    "    \"\"\"\n",
    "    Download historical price data for a list of stocks.\n",
    "    \n",
    "    Args:\n",
    "        tickers (list): List of stock ticker symbols.\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "        save_path (str, optional): Path to save the data. If None, data won't be saved.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the adjusted close prices of the stocks.\n",
    "    \"\"\"\n",
    "    data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "    \n",
    "    # If only one ticker, yfinance doesn't return a DataFrame with ticker columns\n",
    "    if isinstance(data, pd.Series):\n",
    "        data = pd.DataFrame(data, columns=[tickers[0]])\n",
    "    \n",
    "    # Fill missing values using forward fill then backward fill\n",
    "    data = data.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        data.to_csv(save_path)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def calculate_returns(prices, log_returns=False):\n",
    "    \"\"\"\n",
    "    Calculate daily returns from price data.\n",
    "    \n",
    "    Args:\n",
    "        prices (pd.DataFrame): DataFrame with price data.\n",
    "        log_returns (bool): If True, calculate log returns, otherwise simple returns.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with returns.\n",
    "    \"\"\"\n",
    "    if log_returns:\n",
    "        returns = np.log(prices / prices.shift(1))\n",
    "    else:\n",
    "        returns = prices / prices.shift(1) - 1\n",
    "    \n",
    "    # Drop the first row which will have NaN values\n",
    "    return returns.dropna()\n",
    "\n",
    "def split_data(data, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Split data into train, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame to split.\n",
    "        train_ratio (float): Ratio of data to use for training.\n",
    "        val_ratio (float): Ratio of data to use for validation.\n",
    "        test_ratio (float): Ratio of data to use for testing.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_data, val_data, test_data)\n",
    "    \"\"\"\n",
    "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1\"\n",
    "    \n",
    "    n = len(data)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_data = data.iloc[:train_end]\n",
    "    val_data = data.iloc[train_end:val_end]\n",
    "    test_data = data.iloc[val_end:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# ### 2.2 Financial Risk Metrics\n",
    "\n",
    "def calculate_cvar(returns, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the Conditional Value-at-Risk (CVaR) for a series of returns.\n",
    "    \n",
    "    Args:\n",
    "        returns (np.array): Array of returns.\n",
    "        alpha (float): The significance level (e.g., 0.05 for 95% CVaR).\n",
    "        \n",
    "    Returns:\n",
    "        float: The CVaR value.\n",
    "    \"\"\"\n",
    "    # Sort returns in ascending order\n",
    "    sorted_returns = np.sort(returns)\n",
    "    \n",
    "    # Determine the VaR threshold\n",
    "    var_threshold_idx = int(np.ceil(alpha * len(sorted_returns))) - 1\n",
    "    if var_threshold_idx < 0:\n",
    "        var_threshold_idx = 0\n",
    "    \n",
    "    # Calculate CVaR as the mean of returns beyond VaR\n",
    "    cvar = np.mean(sorted_returns[:var_threshold_idx+1])\n",
    "    \n",
    "    return cvar\n",
    "\n",
    "def calculate_sharpe_ratio(returns, risk_free_rate=0.0, periods_per_year=252):\n",
    "    \"\"\"\n",
    "    Calculate the Sharpe ratio for a series of returns.\n",
    "    \n",
    "    Args:\n",
    "        returns (np.array): Array of returns.\n",
    "        risk_free_rate (float): The risk-free rate.\n",
    "        periods_per_year (int): Number of periods per year (e.g., 252 for daily returns).\n",
    "        \n",
    "    Returns:\n",
    "        float: The Sharpe ratio.\n",
    "    \"\"\"\n",
    "    mean_return = np.mean(returns)\n",
    "    std_return = np.std(returns)\n",
    "    \n",
    "    if std_return == 0:\n",
    "        return 0\n",
    "    \n",
    "    sharpe = (mean_return - risk_free_rate) / std_return\n",
    "    annualized_sharpe = sharpe * np.sqrt(periods_per_year)\n",
    "    \n",
    "    return annualized_sharpe\n",
    "\n",
    "def calculate_max_drawdown(portfolio_values):\n",
    "    \"\"\"\n",
    "    Calculate the maximum drawdown of a portfolio.\n",
    "    \n",
    "    Args:\n",
    "        portfolio_values (np.array): Array of portfolio values.\n",
    "        \n",
    "    Returns:\n",
    "        float: Maximum drawdown.\n",
    "    \"\"\"\n",
    "    # Calculate the maximum drawdown\n",
    "    peak = portfolio_values[0]\n",
    "    max_drawdown = 0\n",
    "    \n",
    "    for value in portfolio_values:\n",
    "        if value > peak:\n",
    "            peak = value\n",
    "        drawdown = (peak - value) / peak\n",
    "        max_drawdown = max(max_drawdown, drawdown)\n",
    "    \n",
    "    return max_drawdown\n",
    "\n",
    "def calculate_turnover(old_weights, new_weights):\n",
    "    \"\"\"\n",
    "    Calculate portfolio turnover.\n",
    "    \n",
    "    Args:\n",
    "        old_weights (np.array): Previous portfolio weights.\n",
    "        new_weights (np.array): New portfolio weights.\n",
    "        \n",
    "    Returns:\n",
    "        float: Portfolio turnover.\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(new_weights - old_weights)) / 2.0\n",
    "\n",
    "# ### 2.3 Data Loading and Exploration\n",
    "\n",
    "# Define the assets we want to include in our portfolio\n",
    "tickers = ['SPY', 'QQQ', 'GLD', 'TLT', 'VNQ', 'BND', 'VWO']\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2023-01-01'\n",
    "\n",
    "# Data file path\n",
    "data_file = os.path.join('data', 'stock_data.csv')\n",
    "\n",
    "# Load or download data\n",
    "if os.path.exists(data_file):\n",
    "    print(f\"Loading data from {data_file}\")\n",
    "    price_data = pd.read_csv(data_file, index_col=0, parse_dates=True)\n",
    "else:\n",
    "    print(f\"Downloading data for {tickers}\")\n",
    "    price_data = download_stock_data(tickers, start_date, end_date, save_path=data_file)\n",
    "\n",
    "# Display the first few rows of price data\n",
    "print(\"Price data:\")\n",
    "price_data.head()\n",
    "\n",
    "# Calculate returns\n",
    "returns_data = calculate_returns(price_data, log_returns=False)\n",
    "\n",
    "# Display the first few rows of returns data\n",
    "print(\"Returns data:\")\n",
    "returns_data.head()\n",
    "\n",
    "# Plot the price data\n",
    "plt.figure(figsize=(14, 7))\n",
    "for ticker in tickers:\n",
    "    plt.plot(price_data.index, price_data[ticker] / price_data[ticker].iloc[0], label=ticker)\n",
    "plt.title('Price Evolution (Normalized)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Normalized Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(returns_data.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Asset Returns')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display risk metrics for individual assets\n",
    "risk_metrics = pd.DataFrame(index=tickers)\n",
    "risk_metrics['Annual Return'] = [np.mean(returns_data[ticker]) * 252 for ticker in tickers]\n",
    "risk_metrics['Annual Volatility'] = [np.std(returns_data[ticker]) * np.sqrt(252) for ticker in tickers]\n",
    "risk_metrics['Sharpe Ratio'] = [calculate_sharpe_ratio(returns_data[ticker].values) for ticker in tickers]\n",
    "risk_metrics['CVaR (5%)'] = [calculate_cvar(returns_data[ticker].values) for ticker in tickers]\n",
    "\n",
    "print(\"Risk metrics for individual assets:\")\n",
    "risk_metrics\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_data, val_data, test_data = split_data(returns_data)\n",
    "\n",
    "print(f\"Data shapes - Train: {train_data.shape}, Val: {val_data.shape}, Test: {test_data.shape}\")\n",
    "\n",
    "# ## 3. Portfolio Environment Construction\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A reinforcement learning environment for portfolio management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, returns, features=None, window_size=10, transaction_cost=0.001, \n",
    "                 risk_aversion=1.0, initial_amount=1.0, reward_mode='risk_adjusted'):\n",
    "        \"\"\"\n",
    "        Initialize the environment.\n",
    "        \n",
    "        Args:\n",
    "            returns (pd.DataFrame): DataFrame with asset returns.\n",
    "            features (pd.DataFrame, optional): DataFrame with additional features.\n",
    "            window_size (int): Size of the observation window.\n",
    "            transaction_cost (float): Transaction cost as a fraction of traded amount.\n",
    "            risk_aversion (float): Risk aversion parameter for CVaR penalty.\n",
    "            initial_amount (float): Initial portfolio value.\n",
    "            reward_mode (str): Reward calculation mode ('return', 'sharpe', 'risk_adjusted').\n",
    "        \"\"\"\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        \n",
    "        self.returns = returns.values\n",
    "        self.dates = returns.index\n",
    "        self.asset_names = returns.columns\n",
    "        self.num_assets = returns.shape[1]\n",
    "        self.window_size = window_size\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.risk_aversion = risk_aversion\n",
    "        self.initial_amount = initial_amount\n",
    "        self.reward_mode = reward_mode\n",
    "        \n",
    "        # Set additional features if provided\n",
    "        if features is not None:\n",
    "            self.features = features.values\n",
    "            assert features.shape[0] == returns.shape[0], \"Features and returns must have same number of time steps\"\n",
    "            self.feature_dim = features.shape[1]\n",
    "        else:\n",
    "            self.features = None\n",
    "            self.feature_dim = 0\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        # Action space: portfolio weights (continuous) that sum to 1\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.num_assets,), dtype=np.float32)\n",
    "        \n",
    "        # Observation space: window of past returns and features\n",
    "        obs_dim = self.window_size * self.num_assets\n",
    "        if self.features is not None:\n",
    "            obs_dim += self.feature_dim\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Initialize state\n",
    "        self.reset()\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Construct the observation from the current state.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: The observation.\n",
    "        \"\"\"\n",
    "        # Get the returns window\n",
    "        returns_window = self.returns[self.current_step - self.window_size:self.current_step]\n",
    "        obs = returns_window.flatten()\n",
    "        \n",
    "        # Add features if available\n",
    "        if self.features is not None:\n",
    "            current_features = self.features[self.current_step]\n",
    "            obs = np.concatenate([obs, current_features])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def _calculate_reward(self, portfolio_return, previous_weights, new_weights):\n",
    "        \"\"\"\n",
    "        Calculate the reward based on portfolio return and risk.\n",
    "        \n",
    "        Args:\n",
    "            portfolio_return (float): Portfolio return.\n",
    "            previous_weights (np.array): Previous portfolio weights.\n",
    "            new_weights (np.array): New portfolio weights.\n",
    "            \n",
    "        Returns:\n",
    "            float: The reward.\n",
    "        \"\"\"\n",
    "        # Calculate transaction cost\n",
    "        turnover = np.sum(np.abs(new_weights - previous_weights))\n",
    "        cost = self.transaction_cost * turnover\n",
    "        \n",
    "        # Apply transaction cost to return\n",
    "        net_return = portfolio_return - cost\n",
    "        \n",
    "        if self.reward_mode == 'return':\n",
    "            return net_return\n",
    "        \n",
    "        # For risk-adjusted rewards, we need a window of recent portfolio returns\n",
    "        if self.current_step >= self.window_size + 20:  # Need enough history for meaningful risk calculation\n",
    "            # Calculate portfolio returns over a window\n",
    "            hist_portfolio_returns = np.zeros(20)\n",
    "            for i in range(20):\n",
    "                step = self.current_step - 20 + i\n",
    "                hist_portfolio_returns[i] = np.dot(self.returns[step], new_weights)\n",
    "            \n",
    "            # Calculate CVaR for the window\n",
    "            sorted_returns = np.sort(hist_portfolio_returns)\n",
    "            cvar_threshold_idx = int(0.05 * len(sorted_returns))\n",
    "            cvar = np.mean(sorted_returns[:cvar_threshold_idx+1]) if cvar_threshold_idx >= 0 else sorted_returns[0]\n",
    "            \n",
    "            if self.reward_mode == 'risk_adjusted':\n",
    "                # Risk-adjusted return (return - Î» * CVaR)\n",
    "                return net_return - self.risk_aversion * abs(cvar)\n",
    "            elif self.reward_mode == 'sharpe':\n",
    "                # Approximate Sharpe ratio over the window\n",
    "                mean_return = np.mean(hist_portfolio_returns)\n",
    "                std_return = np.std(hist_portfolio_returns)\n",
    "                return mean_return / (std_return + 1e-6)  # Avoid division by zero\n",
    "        \n",
    "        # Default to net return if not enough history\n",
    "        return net_return\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action in the environment.\n",
    "        \n",
    "        Args:\n",
    "            action (np.array): Portfolio weights.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (observation, reward, terminated, truncated, info)\n",
    "        \"\"\"\n",
    "        # Ensure action is valid (weights sum to 1)\n",
    "        action = np.clip(action, 0, 1)\n",
    "        action_sum = np.sum(action)\n",
    "        if action_sum > 0:\n",
    "            action = action / action_sum\n",
    "        \n",
    "        # Store previous weights and portfolio value\n",
    "        prev_weights = self.weights.copy()\n",
    "        prev_portfolio_value = self.portfolio_value\n",
    "        \n",
    "        # Move to the next time step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check if episode is done\n",
    "        terminated = self.current_step >= len(self.returns) - 1\n",
    "        truncated = False\n",
    "        \n",
    "        # Update weights\n",
    "        self.weights = action\n",
    "        \n",
    "        # Calculate portfolio return\n",
    "        if not terminated:\n",
    "            step_returns = self.returns[self.current_step]\n",
    "            portfolio_return = np.dot(step_returns, self.weights)\n",
    "            self.portfolio_value = prev_portfolio_value * (1 + portfolio_return)\n",
    "            self.portfolio_returns.append(portfolio_return)\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = self._calculate_reward(portfolio_return, prev_weights, self.weights)\n",
    "            \n",
    "            # Get new observation\n",
    "            observation = self._get_observation()\n",
    "            \n",
    "            # Store info for logging\n",
    "            info = {\n",
    "                'portfolio_value': self.portfolio_value,\n",
    "                'portfolio_return': portfolio_return,\n",
    "                'weights': self.weights,\n",
    "                'date': self.dates[self.current_step] if isinstance(self.dates, np.ndarray) else self.dates.iloc[self.current_step]\n",
    "            }\n",
    "        else:\n",
    "            # If done, return the final observation and zero reward\n",
    "            observation = self._get_observation()\n",
    "            reward = 0\n",
    "            info = {\n",
    "                'portfolio_value': self.portfolio_value,\n",
    "                'portfolio_return': 0,\n",
    "                'weights': self.weights,\n",
    "                'date': self.dates[self.current_step] if isinstance(self.dates, np.ndarray) else self.dates.iloc[self.current_step]\n",
    "            }\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Initial observation.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Initialize state variables\n",
    "        self.current_step = self.window_size\n",
    "        self.portfolio_value = self.initial_amount\n",
    "        self.portfolio_returns = []\n",
    "        \n",
    "        # Initialize with equal weights\n",
    "        self.weights = np.ones(self.num_assets) / self.num_assets\n",
    "        \n",
    "        # Get initial observation\n",
    "        observation = self._get_observation()\n",
    "        \n",
    "        info = {}\n",
    "        return observation, info\n",
    "    \n",
    "    def get_portfolio_history(self):\n",
    "        \"\"\"\n",
    "        Get the portfolio value history.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (dates, portfolio_values)\n",
    "        \"\"\"\n",
    "        portfolio_values = [self.initial_amount]\n",
    "        for r in self.portfolio_returns:\n",
    "            portfolio_values.append(portfolio_values[-1] * (1 + r))\n",
    "        \n",
    "        dates = self.dates[self.window_size:self.window_size + len(portfolio_values)]\n",
    "        \n",
    "        return dates, portfolio_values\n",
    "\n",
    "# Create environments\n",
    "window_size = 20\n",
    "transaction_cost = 0.001\n",
    "risk_aversion = 1.0\n",
    "reward_mode = 'risk_adjusted'\n",
    "\n",
    "# Initialize environments for train, validation, and test sets\n",
    "train_env = PortfolioEnv(\n",
    "    returns=train_data,\n",
    "    window_size=window_size,\n",
    "    transaction_cost=transaction_cost,\n",
    "    risk_aversion=risk_aversion,\n",
    "    reward_mode=reward_mode\n",
    ")\n",
    "\n",
    "val_env = PortfolioEnv(\n",
    "    returns=val_data,\n",
    "    window_size=window_size,\n",
    "    transaction_cost=transaction_cost,\n",
    "    risk_aversion=risk_aversion,\n",
    "    reward_mode=reward_mode\n",
    ")\n",
    "\n",
    "test_env = PortfolioEnv(\n",
    "    returns=test_data,\n",
    "    window_size=window_size,\n",
    "    transaction_cost=transaction_cost,\n",
    "    risk_aversion=risk_aversion,\n",
    "    reward_mode=reward_mode\n",
    ")\n",
    "\n",
    "# Get dimensions for neural networks\n",
    "input_dim = train_env.observation_space.shape[0]\n",
    "output_dim = train_env.action_space.shape[0]\n",
    "\n",
    "print(f\"State dimension: {input_dim}, Action dimension: {output_dim}\")\n",
    "\n",
    "# ## 4. REINFORCE Implementation\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy network for the REINFORCE algorithm.\n",
    "    Outputs parameters for a Dirichlet distribution over portfolio weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softplus()  # Ensure positive concentration parameters for Dirichlet\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add a small constant to avoid zero concentration parameters\n",
    "        return self.network(x) + 0.1\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Value network for baseline in REINFORCE algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"\n",
    "    REINFORCE agent with baseline for portfolio optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, lr_policy=0.001, lr_value=0.001, gamma=0.99, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the REINFORCE agent.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Dimension of input features.\n",
    "            output_dim (int): Dimension of output (number of assets).\n",
    "            lr_policy (float): Learning rate for policy network.\n",
    "            lr_value (float): Learning rate for value network.\n",
    "            gamma (float): Discount factor.\n",
    "            device (str): Device to use for tensor operations.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.policy_net = PolicyNetwork(input_dim, output_dim).to(device)\n",
    "        self.value_net = ValueNetwork(input_dim).to(device)\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr_policy)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr_value)\n",
    "        \n",
    "        # For storing episode history\n",
    "        self.reset_episode()\n",
    "    \n",
    "    def reset_episode(self):\n",
    "        \"\"\"\n",
    "        Reset the episode history.\n",
    "        \"\"\"\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        self.actions = []\n",
    "        \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        \"\"\"\n",
    "        Select an action using the policy network.\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Current state.\n",
    "            evaluate (bool): If True, use the mode of the distribution.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Selected action (portfolio weights).\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Get Dirichlet concentration parameters\n",
    "        alpha = self.policy_net(state).squeeze()\n",
    "        \n",
    "        if evaluate:\n",
    "            # During evaluation, use mean of Dirichlet distribution\n",
    "            # E[Dir(alpha)] = alpha / sum(alpha)\n",
    "            action = alpha / alpha.sum()\n",
    "            return action.detach().cpu().numpy()\n",
    "        \n",
    "        # Sample from Dirichlet distribution\n",
    "        m = Dirichlet(alpha)\n",
    "        action = m.sample()\n",
    "        \n",
    "        # Store log probability and entropy for training\n",
    "        log_prob = m.log_prob(action)\n",
    "        entropy = m.entropy()\n",
    "        \n",
    "        # Store value estimate\n",
    "        value = self.value_net(state)\n",
    "        \n",
    "        # Store for training\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.entropies.append(entropy)\n",
    "        self.values.append(value)\n",
    "        self.actions.append(action)\n",
    "        \n",
    "        return action.detach().cpu().numpy()\n",
    "    \n",
    "    def update(self, last_value=0):\n",
    "        \"\"\"\n",
    "        Update the policy and value networks.\n",
    "        \n",
    "        Args:\n",
    "            last_value (float): Value estimate for the final state.\n",
    "            \n",
    "        Returns:\n",
    "            float: Policy loss.\n",
    "            float: Value loss.\n",
    "        \"\"\"\n",
    "        rewards = self.rewards\n",
    "        values = self.values + [torch.tensor([[last_value]], device=self.device)]\n",
    "        \n",
    "        # Calculate returns and advantages\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        R = last_value\n",
    "        \n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + self.gamma * R\n",
    "            advantage = R - values[i].item()\n",
    "            \n",
    "            returns.append(R)\n",
    "            advantages.append(advantage)\n",
    "        \n",
    "        # Reverse the lists to match the original order\n",
    "        returns = returns[::-1]\n",
    "        advantages = advantages[::-1]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        returns = torch.tensor(returns, device=self.device).unsqueeze(1)\n",
    "        advantages = torch.tensor(advantages, device=self.device)\n",
    "        \n",
    "        # Update policy network\n",
    "        policy_loss = 0\n",
    "        for log_prob, advantage, entropy in zip(self.log_probs, advantages, self.entropies):\n",
    "            policy_loss += -log_prob * advantage - 0.01 * entropy  # Add entropy regularization\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)  # Gradient clipping\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        # Update value network\n",
    "        value_loss = F.mse_loss(torch.cat(self.values), returns)\n",
    "        \n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)  # Gradient clipping\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        # Reset episode history\n",
    "        self.reset_episode()\n",
    "        \n",
    "        return policy_loss.item(), value_loss.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Store a reward from the environment.\n",
    "        \n",
    "        Args:\n",
    "            reward (float): Reward from the environment.\n",
    "        \"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the agent's models.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to save the models.\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'value_net': self.value_net.state_dict(),\n",
    "            'policy_optimizer': self.policy_optimizer.state_dict(),\n",
    "            'value_optimizer': self.value_optimizer.state_dict(),\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Load the agent's models.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to load the models from.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.value_net.load_state_dict(checkpoint['value_net'])\n",
    "        self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer'])\n",
    "        self.value_optimizer.load_state_dict(checkpoint['value_optimizer'])\n",
    "\n",
    "# ## 5. Actor-Critic (A2C) Implementation\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic network with shared features.\n",
    "    Actor outputs parameters for Dirichlet distribution over portfolio weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor head (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softplus()  # Ensure positive concentration parameters\n",
    "        )\n",
    "        \n",
    "        # Critic head (value)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Actor output (add small constant to avoid zero concentration)\n",
    "        alpha = self.actor(features) + 0.1\n",
    "        \n",
    "        # Critic output (value)\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return alpha, value\n",
    "\n",
    "\n",
    "class A2CAgent:\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic (A2C) agent for portfolio optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, lr=0.001, gamma=0.99, entropy_coef=0.01, value_coef=0.5, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the A2C agent.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Dimension of input features.\n",
    "            output_dim (int): Dimension of output (number of assets).\n",
    "            lr (float): Learning rate.\n",
    "            gamma (float): Discount factor.\n",
    "            entropy_coef (float): Entropy regularization coefficient.\n",
    "            value_coef (float): Value loss coefficient.\n",
    "            device (str): Device to use for tensor operations.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        \n",
    "        # Initialize network and optimizer\n",
    "        self.network = ActorCritic(input_dim, output_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        # For storing episode history\n",
    "        self.reset_episode()\n",
    "    \n",
    "    def reset_episode(self):\n",
    "        \"\"\"\n",
    "        Reset the episode history.\n",
    "        \"\"\"\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        self.actions = []\n",
    "    \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        \"\"\"\n",
    "        Select an action using the actor-critic network.\n",
    "        \n",
    "        Args:\n",
    "            state (np.array): Current state.\n",
    "            evaluate (bool): If True, use the mode of the distribution.\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Selected action (portfolio weights).\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Get policy parameters and value\n",
    "        alpha, value = self.network(state)\n",
    "        alpha = alpha.squeeze()\n",
    "        \n",
    "        if evaluate:\n",
    "            # During evaluation, use mean of Dirichlet distribution\n",
    "            action = alpha / alpha.sum()\n",
    "            return action.detach().cpu().numpy()\n",
    "        \n",
    "        # Sample from Dirichlet distribution\n",
    "        m = Dirichlet(alpha)\n",
    "        action = m.sample()\n",
    "        \n",
    "        # Store log probability and entropy for training\n",
    "        log_prob = m.log_prob(action)\n",
    "        entropy = m.entropy()\n",
    "        \n",
    "        # Store for training\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        self.entropies.append(entropy)\n",
    "        self.actions.append(action)\n",
    "        \n",
    "        return action.detach().cpu().numpy()\n",
    "    \n",
    "    def update(self, next_value=0):\n",
    "        \"\"\"\n",
    "        Update the actor-critic network.\n",
    "        \n",
    "        Args:\n",
    "            next_value (float): Value estimate for the final state.\n",
    "            \n",
    "        Returns:\n",
    "            float: Total loss.\n",
    "            float: Actor loss.\n",
    "            float: Critic loss.\n",
    "            float: Entropy loss.\n",
    "        \"\"\"\n",
    "        rewards = self.rewards\n",
    "        values = self.values + [torch.tensor([[next_value]], device=self.device)]\n",
    "        \n",
    "        # Calculate returns and advantages using Generalized Advantage Estimation (GAE)\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        for i in reversed(range(len(rewards))):\n",
    "            delta = rewards[i] + self.gamma * values[i+1].item() - values[i].item()\n",
    "            gae = delta + self.gamma * 0.95 * gae  # 0.95 is the GAE lambda\n",
    "            advantage = gae\n",
    "            \n",
    "            returns.append(gae + values[i].item())\n",
    "            advantages.append(advantage)\n",
    "        \n",
    "        # Reverse the lists to match the original order\n",
    "        returns = returns[::-1]\n",
    "        advantages = advantages[::-1]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        returns = torch.tensor(returns, device=self.device).unsqueeze(1)\n",
    "        advantages = torch.tensor(advantages, device=self.device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Calculate actor (policy) loss\n",
    "        actor_loss = 0\n",
    "        for log_prob, advantage in zip(self.log_probs, advantages):\n",
    "            actor_loss += -log_prob * advantage\n",
    "        actor_loss = actor_loss / len(self.log_probs)\n",
    "        \n",
    "        # Calculate critic (value) loss\n",
    "        critic_loss = F.mse_loss(torch.cat(self.values), returns)\n",
    "        \n",
    "        # Calculate entropy loss (for exploration)\n",
    "        entropy_loss = -torch.stack(self.entropies).mean()\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = actor_loss + self.value_coef * critic_loss + self.entropy_coef * entropy_loss\n",
    "        \n",
    "        # Update network\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)  # Gradient clipping\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Reset episode history\n",
    "        self.reset_episode()\n",
    "        \n",
    "        return (\n",
    "            total_loss.item(),\n",
    "            actor_loss.item(),\n",
    "            critic_loss.item(),\n",
    "            entropy_loss.item()\n",
    "        )\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Store a reward from the environment.\n",
    "        \n",
    "        Args:\n",
    "            reward (float): Reward from the environment.\n",
    "        \"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the agent's model.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to save the model.\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'network': self.network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Load the agent's model.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to load the model from.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.network.load_state_dict(checkpoint['network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# ## 6. Training and Evaluation Framework\n",
    "\n",
    "def evaluate_agent(env, agent, num_episodes=5):\n",
    "    \"\"\"\n",
    "    Evaluate an agent on multiple episodes.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment.\n",
    "        agent: The agent.\n",
    "        num_episodes (int): Number of episodes to evaluate on.\n",
    "        \n",
    "    Returns:\n",
    "        float: Average reward over episodes.\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done and not truncated:\n",
    "            action = agent.select_action(state, evaluate=True)\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "        \n",
    "        total_reward += episode_reward\n",
    "    \n",
    "    return total_reward / num_episodes\n",
    "\n",
    "def train_reinforce(env, agent, num_episodes, eval_interval=20, eval_episodes=5, save_path=None):\n",
    "    \"\"\"\n",
    "    Train a REINFORCE agent.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment.\n",
    "        agent: The REINFORCE agent.\n",
    "        num_episodes (int): Number of episodes to train for.\n",
    "        eval_interval (int): Interval for evaluation during training.\n",
    "        eval_episodes (int): Number of episodes for evaluation.\n",
    "        save_path (str): Path to save the trained agent.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (agent, training_rewards, evaluation_rewards)\n",
    "    \"\"\"\n",
    "    training_rewards = []\n",
    "    evaluation_rewards = []\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        # Reset the environment and agent episode history\n",
    "        state, _ = env.reset()\n",
    "        agent.reset_episode()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Run an episode\n",
    "        while not done and not truncated:\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Store reward\n",
    "            agent.store_reward(reward)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Update agent after episode\n",
    "        agent.update()\n",
    "        \n",
    "        # Record training reward\n",
    "        training_rewards.append(episode_reward)\n",
    "        \n",
    "        # Evaluate the agent periodically\n",
    "        if (episode + 1) % eval_interval == 0:\n",
    "            eval_reward = evaluate_agent(env, agent, eval_episodes)\n",
    "            evaluation_rewards.append(eval_reward)\n",
    "            \n",
    "            print(f\"\\nEpisode {episode+1}/{num_episodes}\")\n",
    "            print(f\"Training reward: {episode_reward:.4f}\")\n",
    "            print(f\"Evaluation reward: {eval_reward:.4f}\")\n",
    "            \n",
    "            # Save the best agent\n",
    "            if save_path and (len(evaluation_rewards) == 1 or eval_reward > max(evaluation_rewards[:-1])):\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                agent.save(save_path)\n",
    "                print(f\"Saved model to {save_path}\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(training_rewards, label='Training Rewards')\n",
    "    plt.plot(np.arange(eval_interval-1, num_episodes, eval_interval), evaluation_rewards, label='Evaluation Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('REINFORCE Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return agent, training_rewards, evaluation_rewards\n",
    "\n",
    "def train_a2c(env, agent, num_episodes, eval_interval=20, eval_episodes=5, save_path=None):\n",
    "    \"\"\"\n",
    "    Train an A2C agent.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment.\n",
    "        agent: The A2C agent.\n",
    "        num_episodes (int): Number of episodes to train for.\n",
    "        eval_interval (int): Interval for evaluation during training.\n",
    "        eval_episodes (int): Number of episodes for evaluation.\n",
    "        save_path (str): Path to save the trained agent.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (agent, training_rewards, evaluation_rewards)\n",
    "    \"\"\"\n",
    "    training_rewards = []\n",
    "    evaluation_rewards = []\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        # Reset the environment and agent episode history\n",
    "        state, _ = env.reset()\n",
    "        agent.reset_episode()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Run an episode\n",
    "        while not done and not truncated:\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Store reward\n",
    "            agent.store_reward(reward)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Update agent after episode\n",
    "        total_loss, actor_loss, critic_loss, entropy_loss = agent.update()\n",
    "        \n",
    "        # Record training reward\n",
    "        training_rewards.append(episode_reward)\n",
    "        \n",
    "        # Evaluate the agent periodically\n",
    "        if (episode + 1) % eval_interval == 0:\n",
    "            eval_reward = evaluate_agent(env, agent, eval_episodes)\n",
    "            evaluation_rewards.append(eval_reward)\n",
    "            \n",
    "            print(f\"\\nEpisode {episode+1}/{num_episodes}\")\n",
    "            print(f\"Training reward: {episode_reward:.4f}\")\n",
    "            print(f\"Evaluation reward: {eval_reward:.4f}\")\n",
    "            print(f\"Actor loss: {actor_loss:.4f}, Critic loss: {critic_loss:.4f}, Entropy loss: {entropy_loss:.4f}\")\n",
    "            \n",
    "            # Save the best agent\n",
    "            if save_path and (len(evaluation_rewards) == 1 or eval_reward > max(evaluation_rewards[:-1])):\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                agent.save(save_path)\n",
    "                print(f\"Saved model to {save_path}\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(training_rewards, label='Training Rewards')\n",
    "    plt.plot(np.arange(eval_interval-1, num_episodes, eval_interval), evaluation_rewards, label='Evaluation Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('A2C Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return agent, training_rewards, evaluation_rewards\n",
    "\n",
    "# ## 7. Train REINFORCE Agent\n",
    "\n",
    "# Initialize REINFORCE agent\n",
    "reinforce_agent = REINFORCEAgent(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    lr_policy=0.001,\n",
    "    lr_value=0.001,\n",
    "    gamma=0.99,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Define save path\n",
    "reinforce_path = os.path.join('saved_models', 'reinforce_agent.pth')\n",
    "\n",
    "# Train the agent (uncomment to run training)\n",
    "# reinforce_agent, reinforce_train_rewards, reinforce_eval_rewards = train_reinforce(\n",
    "#     env=train_env,\n",
    "#     agent=reinforce_agent,\n",
    "#     num_episodes=200,\n",
    "#     eval_interval=20,\n",
    "#     save_path=reinforce_path\n",
    "# )\n",
    "\n",
    "# ## 8. Train A2C Agent\n",
    "\n",
    "# Initialize A2C agent\n",
    "a2c_agent = A2CAgent(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    lr=0.001,\n",
    "    gamma=0.99,\n",
    "    entropy_coef=0.01,\n",
    "    value_coef=0.5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Define save path\n",
    "a2c_path = os.path.join('saved_models', 'a2c_agent.pth')\n",
    "\n",
    "# Train the agent (uncomment to run training)\n",
    "# a2c_agent, a2c_train_rewards, a2c_eval_rewards = train_a2c(\n",
    "#     env=train_env,\n",
    "#     agent=a2c_agent,\n",
    "#     num_episodes=200,\n",
    "#     eval_interval=20,\n",
    "#     save_path=a2c_path\n",
    "# )\n",
    "\n",
    "# ## 9. Performance Evaluation and Analysis\n",
    "\n",
    "def evaluate_portfolio(env, agent, returns_df, plot=True, title=None):\n",
    "    \"\"\"\n",
    "    Evaluate a trained agent on a portfolio environment.\n",
    "    \n",
    "    Args:\n",
    "        env: The portfolio environment.\n",
    "        agent: The trained agent.\n",
    "        returns_df (pd.DataFrame): DataFrame with returns data.\n",
    "        plot (bool): Whether to plot the performance.\n",
    "        title (str): Title for the plot.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Performance metrics.\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    \n",
    "    # Track portfolio values and weights\n",
    "    portfolio_values = [env.portfolio_value]\n",
    "    weights_history = []\n",
    "    dates = []\n",
    "    \n",
    "    while not done and not truncated:\n",
    "        action = agent.select_action(obs, evaluate=True)\n",
    "        obs, _, done, truncated, info = env.step(action)\n",
    "        \n",
    "        portfolio_values.append(info['portfolio_value'])\n",
    "        weights_history.append(info['weights'])\n",
    "        dates.append(info['date'])\n",
    "    \n",
    "    # Convert to arrays and DataFrames\n",
    "    portfolio_values = np.array(portfolio_values)\n",
    "    weights_history = np.array(weights_history)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    portfolio_returns = np.diff(portfolio_values) / portfolio_values[:-1]\n",
    "    \n",
    "    total_return = (portfolio_values[-1] / portfolio_values[0]) - 1\n",
    "    annualized_return = (1 + total_return) ** (252 / len(portfolio_returns)) - 1\n",
    "    sharpe = calculate_sharpe_ratio(portfolio_returns)\n",
    "    cvar = calculate_cvar(portfolio_returns)\n",
    "    volatility = np.std(portfolio_returns) * np.sqrt(252)\n",
    "    max_drawdown = calculate_max_drawdown(portfolio_values)\n",
    "    \n",
    "    # Calculate turnover\n",
    "    turnover = 0\n",
    "    for i in range(1, len(weights_history)):\n",
    "        turnover += np.sum(np.abs(weights_history[i] - weights_history[i-1])) / 2.0\n",
    "    avg_turnover = turnover / (len(weights_history) - 1) if len(weights_history) > 1 else 0\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        'total_return': total_return,\n",
    "        'annualized_return': annualized_return,\n",
    "        'sharpe_ratio': sharpe,\n",
    "        'cvar': cvar,\n",
    "        'volatility': volatility,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'avg_turnover': avg_turnover\n",
    "    }\n",
    "    \n",
    "    if plot:\n",
    "        # Plot portfolio performance\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), gridspec_kw={'height_ratios': [2, 1]})\n",
    "        \n",
    "        # Plot portfolio value\n",
    "        ax1.plot(dates, portfolio_values, label='Portfolio Value')\n",
    "        \n",
    "        if title:\n",
    "            ax1.set_title(title)\n",
    "        else:\n",
    "            ax1.set_title('Portfolio Performance')\n",
    "            \n",
    "        ax1.set_ylabel('Portfolio Value')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot asset weights over time\n",
    "        if len(weights_history) > 0:\n",
    "            df_weights = pd.DataFrame(weights_history, columns=env.asset_names, index=dates)\n",
    "            df_weights.plot(kind='area', stacked=True, ax=ax2)\n",
    "            ax2.set_title('Asset Allocation Over Time')\n",
    "            ax2.set_ylabel('Weight')\n",
    "            ax2.set_xlabel('Date')\n",
    "            ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(\"\\nPerformance Metrics:\")\n",
    "        print(f\"Total Return: {total_return:.4f} ({annualized_return:.4f} annualized)\")\n",
    "        print(f\"Sharpe Ratio: {sharpe:.4f}\")\n",
    "        print(f\"CVaR (5%): {cvar:.4f}\")\n",
    "        print(f\"Volatility (annualized): {volatility:.4f}\")\n",
    "        print(f\"Maximum Drawdown: {max_drawdown:.4f}\")\n",
    "        print(f\"Average Turnover: {avg_turnover:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def compare_agents(envs, agents, agent_names, returns_df, plot=True):\n",
    "    \"\"\"\n",
    "    Compare multiple agents on the same environment.\n",
    "    \n",
    "    Args:\n",
    "        envs (list): List of portfolio environments.\n",
    "        agents (list): List of trained agents.\n",
    "        agent_names (list): List of agent names.\n",
    "        returns_df (pd.DataFrame): DataFrame with returns data.\n",
    "        plot (bool): Whether to plot the comparison.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Performance metrics for each agent.\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "    portfolio_values_dict = {}\n",
    "    \n",
    "    for i, (env, agent, name) in enumerate(zip(envs, agents, agent_names)):\n",
    "        metrics = evaluate_portfolio(env, agent, returns_df, plot=False)\n",
    "        metrics['agent'] = name\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        # Track portfolio values\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        portfolio_values = [env.portfolio_value]\n",
    "        dates = []\n",
    "        \n",
    "        while not done and not truncated:\n",
    "            action = agent.select_action(obs, evaluate=True)\n",
    "            obs, _, done, truncated, info = env.step(action)\n",
    "            portfolio_values.append(info['portfolio_value'])\n",
    "            if i == 0:  # Only need to save dates once\n",
    "                dates.append(info['date'])\n",
    "                \n",
    "        portfolio_values_dict[name] = portfolio_values\n",
    "    \n",
    "    # Create DataFrame with metrics\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    \n",
    "    if plot:\n",
    "        # Plot portfolio values\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for name, values in portfolio_values_dict.items():\n",
    "            plt.plot(dates[:len(values)], values, label=name)\n",
    "            \n",
    "        plt.title('Portfolio Performance Comparison')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Portfolio Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot metrics comparison\n",
    "        metrics_for_plot = ['annualized_return', 'sharpe_ratio', 'cvar', 'volatility', 'max_drawdown', 'avg_turnover']\n",
    "        metrics_df_for_plot = metrics_df.set_index('agent')[metrics_for_plot]\n",
    "        \n",
    "        plt.figure(figsize=(14, 7))\n",
    "        sns.heatmap(metrics_df_for_plot, annot=True, cmap='coolwarm', fmt='.4f', cbar=True)\n",
    "        plt.title('Performance Metrics Comparison')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Load agents if they were trained and saved previously\n",
    "try:\n",
    "    reinforce_agent = REINFORCEAgent(input_dim, output_dim, device=device)\n",
    "    reinforce_agent.load(reinforce_path)\n",
    "    print(\"Loaded REINFORCE agent from saved model\")\n",
    "except:\n",
    "    print(\"No saved REINFORCE model found. Please train the model first.\")\n",
    "\n",
    "try:\n",
    "    a2c_agent = A2CAgent(input_dim, output_dim, device=device)\n",
    "    a2c_agent.load(a2c_path)\n",
    "    print(\"Loaded A2C agent from saved model\")\n",
    "except:\n",
    "    print(\"No saved A2C model found. Please train the model first.\")\n",
    "\n",
    "# ## 10. Stress Testing on Crisis Periods\n",
    "\n",
    "def get_crisis_periods():\n",
    "    \"\"\"\n",
    "    Return predefined crisis periods for testing.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with crisis periods.\n",
    "    \"\"\"\n",
    "    periods = {\n",
    "        'financial_crisis_2008': {\n",
    "            'start': '2008-01-01',\n",
    "            'end': '2009-06-30'\n",
    "        },\n",
    "        'covid_crash_2020': {\n",
    "            'start': '2020-02-01',\n",
    "            'end': '2020-05-31'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return periods\n",
    "\n",
    "def stress_test_performance(env, agent, crisis_periods, returns_df):\n",
    "    \"\"\"\n",
    "    Evaluate agent performance during crisis periods.\n",
    "    \n",
    "    Args:\n",
    "        env: The portfolio environment.\n",
    "        agent: The trained agent.\n",
    "        crisis_periods (dict): Dictionary with crisis period date ranges.\n",
    "        returns_df (pd.DataFrame): DataFrame with returns data.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Performance metrics during crisis periods.\n",
    "    \"\"\"\n",
    "    crisis_metrics = {}\n",
    "    \n",
    "    for period_name, period_dates in crisis_periods.items():\n",
    "        start_date = pd.Timestamp(period_dates['start'])\n",
    "        end_date = pd.Timestamp(period_dates['end'])\n",
    "        \n",
    "        # Filter returns for the crisis period\n",
    "        crisis_returns = returns_df.loc[start_date:end_date] if start_date in returns_df.index and end_date in returns_df.index else pd.DataFrame()\n",
    "        \n",
    "        if len(crisis_returns) == 0:\n",
    "            print(f\"No data available for period: {period_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Create a new environment with crisis period data\n",
    "        crisis_env = PortfolioEnv(\n",
    "            returns=crisis_returns,\n",
    "            window_size=env.window_size,\n",
    "            transaction_cost=env.transaction_cost,\n",
    "            risk_aversion=env.risk_aversion,\n",
    "            initial_amount=env.initial_amount,\n",
    "            reward_mode=env.reward_mode\n",
    "        )\n",
    "        \n",
    "        # Evaluate agent during the crisis period\n",
    "        metrics = evaluate_portfolio(\n",
    "            crisis_env, agent, crisis_returns, \n",
    "            plot=True, \n",
    "            title=f'Portfolio Performance During {period_name}'\n",
    "        )\n",
    "        \n",
    "        crisis_metrics[period_name] = metrics\n",
    "    \n",
    "    return crisis_metrics\n",
    "\n",
    "# Perform stress testing if agents are loaded\n",
    "# Note: This requires downloading additional data for the crisis periods\n",
    "# Uncomment to run stress testing\n",
    "\n",
    "# Download data for crisis periods\n",
    "# crisis_periods = get_crisis_periods()\n",
    "# tickers = ['SPY', 'QQQ', 'GLD', 'TLT', 'VNQ', 'BND', 'VWO']\n",
    "\n",
    "# crisis_start = min([pd.Timestamp(period['start']) for period in crisis_periods.values()])\n",
    "# crisis_end = max([pd.Timestamp(period['end']) for period in crisis_periods.values()])\n",
    "# crisis_data = download_stock_data(tickers, crisis_start.strftime('%Y-%m-%d'), crisis_end.strftime('%Y-%m-%d'))\n",
    "# crisis_returns = calculate_returns(crisis_data)\n",
    "\n",
    "# if 'reinforce_agent' in locals():\n",
    "#     print(\"\\nStress testing REINFORCE agent\")\n",
    "#     reinforce_crisis_metrics = stress_test_performance(test_env, reinforce_agent, crisis_periods, crisis_returns)\n",
    "\n",
    "# if 'a2c_agent' in locals():\n",
    "#     print(\"\\nStress testing A2C agent\")\n",
    "#     a2c_crisis_metrics = stress_test_performance(test_env, a2c_agent, crisis_periods, crisis_returns)\n",
    "\n",
    "# ## 11. Sensitivity Analysis to Risk Aversion Parameter\n",
    "\n",
    "def risk_aversion_sensitivity(env_template, agent, returns_df, risk_aversions=[0.0, 0.5, 1.0, 2.0, 5.0]):\n",
    "    \"\"\"\n",
    "    Analyze sensitivity to the risk aversion parameter.\n",
    "    \n",
    "    Args:\n",
    "        env_template: Template environment to clone with different risk aversion values.\n",
    "        agent: The agent to evaluate.\n",
    "        returns_df (pd.DataFrame): DataFrame with returns data.\n",
    "        risk_aversions (list): List of risk aversion values to test.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Performance metrics for different risk aversion values.\n",
    "    \"\"\"\n",
    "    metrics_list = []\n",
    "    \n",
    "    for risk_aversion in risk_aversions:\n",
    "        # Create environment with the current risk aversion\n",
    "        env = PortfolioEnv(\n",
    "            returns=returns_df,\n",
    "            window_size=env_template.window_size,\n",
    "            transaction_cost=env_template.transaction_cost,\n",
    "            risk_aversion=risk_aversion,\n",
    "            initial_amount=env_template.initial_amount,\n",
    "            reward_mode='risk_adjusted'  # Always use risk-adjusted reward for this analysis\n",
    "        )\n",
    "        \n",
    "        # Evaluate the agent\n",
    "        metrics = evaluate_portfolio(\n",
    "            env, agent, returns_df, \n",
    "            plot=False\n",
    "        )\n",
    "        \n",
    "        metrics['risk_aversion'] = risk_aversion\n",
    "        metrics_list.append(metrics)\n",
    "    \n",
    "    # Create DataFrame with metrics\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    \n",
    "    # Plot metrics vs. risk aversion\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    metrics_to_plot = ['annualized_return', 'sharpe_ratio', 'volatility', 'cvar']\n",
    "    titles = ['Annualized Return', 'Sharpe Ratio', 'Volatility', 'CVaR']\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "        axes[i].plot(metrics_df['risk_aversion'], metrics_df[metric], marker='o')\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].set_xlabel('Risk Aversion Parameter')\n",
    "        axes[i].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Perform risk aversion sensitivity analysis if an agent is loaded\n",
    "# Uncomment to run sensitivity analysis\n",
    "\n",
    "# if 'reinforce_agent' in locals():\n",
    "#     print(\"\\nRisk Aversion Sensitivity Analysis for REINFORCE Agent\")\n",
    "#     reinforce_sensitivity = risk_aversion_sensitivity(test_env, reinforce_agent, test_data)\n",
    "\n",
    "# if 'a2c_agent' in locals():\n",
    "#     print(\"\\nRisk Aversion Sensitivity Analysis for A2C Agent\")\n",
    "#     a2c_sensitivity = risk_aversion_sensitivity(test_env, a2c_agent, test_data)\n",
    "\n",
    "# ## 12. Conclusion\n",
    "\n",
    "# Summary of the portfolio management project using reinforcement learning:\n",
    "# \n",
    "# 1. We implemented REINFORCE and A2C algorithms for portfolio optimization\n",
    "# 2. We incorporated risk-sensitivity through CVaR penalty in the reward function\n",
    "# 3. We evaluated performance on historical data and during crisis periods\n",
    "# 4. We analyzed the sensitivity to risk aversion parameter\n",
    "# \n",
    "# Key findings:\n",
    "# - Actor-Critic methods typically converge faster than REINFORCE\n",
    "# - Higher risk aversion leads to more conservative portfolios with lower volatility\n",
    "# - The agents can adapt to different market regimes, including crisis periods\n",
    "# - Transaction costs significantly impact overall performance\n",
    "# \n",
    "# Future improvements:\n",
    "# - Implement more advanced policy gradient methods (PPO, SAC)\n",
    "# - Add more sophisticated features (technical indicators, sentiment, etc.)\n",
    "# - Incorporate transaction cost optimization directly into the learning objective\n",
    "# - Test with different risk measures beyond CVaR"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
